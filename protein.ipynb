{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Activation, add, Add, Dropout, Concatenate, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "from PIL import Image\n",
    "#from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "random_state = 42\n",
    "THRESHOLD = 0.05\n",
    "\n",
    "def resblock(x, filters=64, kernel_size=(3, 3), activation='relu'):\n",
    "    x_ = Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x_ = BatchNormalization()(x_)\n",
    "    x_ = Conv2D(filters, kernel_size, padding='same')(x_)\n",
    "    x = Add()([x_, x])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def resblock2(x, filters=64, kernel_size=(3, 3), activation='relu'):\n",
    "    x = Conv2D(filters, (1,1), padding='same')(x) \n",
    "    x_ = BatchNormalization()(x)\n",
    "    x_ = Conv2D(filters, (1,1), padding='same')(x_)\n",
    "    x_ = BatchNormalization()(x_)\n",
    "    x_ = Activation(activation)(x_)\n",
    "    x_ = Conv2D(filters, kernel_size, padding='same')(x_)\n",
    "    x_ = BatchNormalization()(x_)\n",
    "    x_ = Activation(activation)(x_)\n",
    "    x_ = Conv2D(filters, (1,1), padding='same')(x_)\n",
    "    x_ = BatchNormalization()(x_)\n",
    "    x = Add()([x_, x])\n",
    "    return x\n",
    "\n",
    "def resnet2(x, filters=64, kernel_size=(3, 3), depth=10):\n",
    "    for i in range(depth):\n",
    "        x = resblock2(x, filters=filters)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, input_channels, growth_rate, nb_blocks):\n",
    "    n_channels = input_channels\n",
    "    orig = x\n",
    "    for i in range(nb_blocks):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.1)(x) #Activation(\"relu\")(x)\n",
    "        x = Conv2D(128, (1, 1), kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.1)(x) #Activation(\"relu\")(x)\n",
    "        # フィルター数 = 成長度合\n",
    "        x = Conv2D(growth_rate, (3, 3), padding=\"same\", kernel_initializer='he_normal')(x)\n",
    "        # origと結合\n",
    "        x = Concatenate()([orig, x])\n",
    "        n_channels += growth_rate\n",
    "    return x, n_channels\n",
    "\n",
    "def transition_layer(x, input_channels, compression_factor=0.5):\n",
    "    n_channels = int(input_channels * compression_factor)\n",
    "    x = Conv2D(n_channels, (1, 1))(x)\n",
    "    x = AveragePooling2D((2, 2))(x)\n",
    "    return x, n_channels\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        eps =1e-12\n",
    "        y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "#        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def darknet_block(x, input_channels):\n",
    "    ch_hid = input_channels//2\n",
    "    x = Conv2D(ch_hid, (1,1), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Conv2D(input_channels, (3,3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    return x\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    eps =1e-12\n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "    zero_weight= 0.2\n",
    "    one_weight=0.8\n",
    "    # Original binary crossentropy (see losses.py):\n",
    "    # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "    # Calculate the binary crossentropy\n",
    "    b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Apply the weights\n",
    "    weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "    weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "    # Return the mean error\n",
    "    return K.mean(weighted_b_ce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/train.csv\", engine='python')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#毎回データをgenerateする関数\n",
    "def generateData(zippath, xs, ys,batch_size=128, imsize=128):\n",
    "    n_batches = math.ceil(len(xs) / batch_size)\n",
    "    while True:   \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            ft = True\n",
    "            z = zipfile.ZipFile(zippath)\n",
    "            for dat, category in zip(xs[start:end], ys[start:end].str.split(' ')):\n",
    "                imgname_red = dat+'_red.png'\n",
    "                img_red = np.array(Image.open( io.BytesIO(z.read(imgname_red)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_red = img_red/np.max(img_red)\n",
    "                imgname_blue = dat+'_blue.png'\n",
    "                img_blue = np.array(Image.open( io.BytesIO(z.read(imgname_blue)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_blue = img_blue/np.max(img_blue)\n",
    "                imgname_green = dat+'_green.png'\n",
    "                img_green = np.array(Image.open( io.BytesIO(z.read(imgname_green)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_green = img_green/np.max(img_green)\n",
    "                imgname_yellow = dat+'_yellow.png'\n",
    "                img_yellow = np.array(Image.open( io.BytesIO(z.read(imgname_yellow)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_yellow = img_yellow/np.max(img_yellow)\n",
    "                #define as numpy 512 x 512 x 4\n",
    "                image_merged = np.concatenate((img_red[:,:,np.newaxis],img_blue[:,:,np.newaxis],\n",
    "                                               img_green[:,:,np.newaxis],img_yellow[:,:,np.newaxis]), axis=-1)\n",
    "                                \n",
    "                #get category\n",
    "                y = np.zeros(28)\n",
    "                for key in category:\n",
    "                    y[int(key)] = 1\n",
    "                    \n",
    "                #append as one numpy (for saving)\n",
    "                if ft:\n",
    "                    image_all = image_merged[np.newaxis,:]\n",
    "                    category_all = y[np.newaxis,:]\n",
    "                    ft = False\n",
    "                else:\n",
    "                    image_all = np.append(image_all, image_merged[np.newaxis, :], axis=0)\n",
    "                    category_all = np.append(category_all, y[np.newaxis, :], axis=0)\n",
    "            z.close()\n",
    "#            print(sys.getsizeof(image_all), sys.getsizeof(category_all), image_all.shape)\n",
    "            yield [image_all, category_all]\n",
    "    \n",
    "    \n",
    "#毎回データをgenerateする関数\n",
    "def generateDatav2(zippath, xs, ys,batch_size=128):\n",
    "    n_batches = math.ceil(len(xs) / batch_size)\n",
    "    while True:   \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            ft = True\n",
    "            z = zipfile.ZipFile(zippath)\n",
    "            for dat, category in zip(xs[start:end], ys[start:end].str.split(' ')):\n",
    "                imgname_green = dat+'_green.png'\n",
    "                img_green = np.array(Image.open( io.BytesIO(z.read(imgname_green)) )).astype('float32')\n",
    "                img_green = img_green/np.max(img_green)\n",
    "                img_green = img_green.reshape(1, 512, 512, 1)\n",
    "                imgname_red = dat+'_red.png'\n",
    "                img_red = np.array(Image.open( io.BytesIO(z.read(imgname_red)) )).astype('float32')\n",
    "                img_red = img_red/np.max(img_red)\n",
    "                img_red = img_red.reshape(1, 512, 512, 1)\n",
    "                imgname_blue = dat+'_blue.png'\n",
    "                img_blue = np.array(Image.open( io.BytesIO(z.read(imgname_blue)) )).astype('float32')\n",
    "                img_blue = img_blue/np.max(img_blue)\n",
    "                img_blue = img_blue.reshape(1, 512, 512, 1)\n",
    "                imgname_yellow = dat+'_yellow.png'\n",
    "                img_yellow = np.array(Image.open( io.BytesIO(z.read(imgname_yellow)) )).astype('float32')\n",
    "                img_yellow = img_yellow/np.max(img_yellow)\n",
    "                img_yellow = img_yellow.reshape(1, 512, 512, 1)\n",
    "                                \n",
    "                #get category\n",
    "                y = np.zeros(28)\n",
    "                for key in category:\n",
    "                    y[int(key)] = 1\n",
    "                    \n",
    "                #append as one numpy (for saving)\n",
    "                if ft:\n",
    "                    img_green_batch = img_green\n",
    "                    img_red_batch = img_red\n",
    "                    img_blue_batch = img_blue\n",
    "                    img_yellow_batch = img_yellow\n",
    "                    category_all = y[np.newaxis,:]\n",
    "                    ft = False\n",
    "                else:\n",
    "                    img_green_batch = np.append(img_green_batch, img_green, axis=0)\n",
    "                    img_red_batch = np.append(img_red_batch, img_red, axis=0)\n",
    "                    img_blue_batch = np.append(img_blue_batch, img_blue, axis=0)\n",
    "                    img_yellow_batch = np.append(img_yellow_batch, img_yellow, axis=0)\n",
    "                    category_all = np.append(category_all, y[np.newaxis, :], axis=0)\n",
    "            z.close()\n",
    "#            print(sys.getsizeof(image_all), sys.getsizeof(category_all), image_all.shape)\n",
    "            yield [img_green_batch, img_red_batch, img_blue_batch , img_yellow_batch], category_all\n",
    "    \n",
    "def generateDatav2aug(zippath, xs, ys,batch_size=128):\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "\n",
    "    n_batches = math.ceil(len(xs) / batch_size)\n",
    "    while True:   \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            ft = True\n",
    "            z = zipfile.ZipFile(zippath)\n",
    "            for dat, category in zip(xs[start:end], ys[start:end].str.split(' ')):\n",
    "                imgname_green = dat+'_green.png'\n",
    "                img_green = np.array(Image.open( io.BytesIO(z.read(imgname_green)) )).astype('float32')\n",
    "                img_green = img_green/np.max(img_green)\n",
    "                img_green = img_green.reshape(512, 512, 1)\n",
    "                imgname_red = dat+'_red.png'\n",
    "                img_red = np.array(Image.open( io.BytesIO(z.read(imgname_red)) )).astype('float32')\n",
    "                img_red = img_red/np.max(img_red)\n",
    "                img_red = img_red.reshape(512, 512, 1)\n",
    "                imgname_blue = dat+'_blue.png'\n",
    "                img_blue = np.array(Image.open( io.BytesIO(z.read(imgname_blue)) )).astype('float32')\n",
    "                img_blue = img_blue/np.max(img_blue)\n",
    "                img_blue = img_blue.reshape(512, 512, 1)\n",
    "                imgname_yellow = dat+'_yellow.png'\n",
    "                img_yellow = np.array(Image.open( io.BytesIO(z.read(imgname_yellow)) )).astype('float32')\n",
    "                img_yellow = img_yellow/np.max(img_yellow)\n",
    "                img_yellow = img_yellow.reshape(512, 512, 1)\n",
    "                \n",
    "                img_green = augment(img_green).reshape(1, 512, 512, 1)\n",
    "                img_red = augment(img_red).reshape(1, 512, 512, 1)\n",
    "                img_blue = augment(img_blue).reshape(1, 512, 512, 1)\n",
    "                img_yellow = augment(img_yellow).reshape(1, 512, 512, 1)\n",
    "                \n",
    "                #get category\n",
    "                y = np.zeros(28)\n",
    "                for key in category:\n",
    "                    y[int(key)] = 1\n",
    "                    \n",
    "                #append as one numpy (for saving)\n",
    "                if ft:\n",
    "                    img_green_batch = img_green\n",
    "                    img_red_batch = img_red\n",
    "                    img_blue_batch = img_blue\n",
    "                    img_yellow_batch = img_yellow\n",
    "                    category_all = y[np.newaxis,:]\n",
    "                    ft = False\n",
    "                else:\n",
    "                    img_green_batch = np.append(img_green_batch, img_green, axis=0)\n",
    "                    img_red_batch = np.append(img_red_batch, img_red, axis=0)\n",
    "                    img_blue_batch = np.append(img_blue_batch, img_blue, axis=0)\n",
    "                    img_yellow_batch = np.append(img_yellow_batch, img_yellow, axis=0)\n",
    "                    category_all = np.append(category_all, y[np.newaxis, :], axis=0)\n",
    "            z.close()\n",
    "#            print(sys.getsizeof(image_all), sys.getsizeof(category_all), image_all.shape)\n",
    "            yield [img_green_batch, img_red_batch, img_blue_batch , img_yellow_batch], category_all\n",
    "\n",
    "def generateDataaug(zippath, xs, ys,batch_size=256, imsize=128):\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    \n",
    "    n_batches = math.ceil(len(xs) / batch_size)\n",
    "    while True:   \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            ft = True\n",
    "            z = zipfile.ZipFile(zippath)\n",
    "            for dat, category in zip(xs[start:end], ys[start:end].str.split(' ')):\n",
    "                imgname_red = dat+'_red.png'\n",
    "                img_red = np.array(Image.open( io.BytesIO(z.read(imgname_red)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_red = img_red/np.max(img_red)\n",
    "                imgname_blue = dat+'_blue.png'\n",
    "                img_blue = np.array(Image.open( io.BytesIO(z.read(imgname_blue)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_blue = img_blue/np.max(img_blue)\n",
    "                imgname_green = dat+'_green.png'\n",
    "                img_green = np.array(Image.open( io.BytesIO(z.read(imgname_green)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_green = img_green/np.max(img_green)\n",
    "                imgname_yellow = dat+'_yellow.png'\n",
    "                img_yellow = np.array(Image.open( io.BytesIO(z.read(imgname_yellow)) ).resize((imsize, imsize))).astype('float32')\n",
    "                img_yellow = img_yellow/np.max(img_yellow)\n",
    "                #define as numpy 512 x 512 x 4\n",
    "                image_merged = np.concatenate((img_red[:,:,np.newaxis],img_blue[:,:,np.newaxis],\n",
    "                                               img_green[:,:,np.newaxis],img_yellow[:,:,np.newaxis]), axis=-1)\n",
    "                \n",
    "                #get category\n",
    "                y = np.zeros(28)\n",
    "                for key in category:\n",
    "                    y[int(key)] = 1\n",
    "                image_merged = augment(image_merged)\n",
    "                #append as one numpy (for saving)\n",
    "                if ft:\n",
    "                    image_all = image_merged[np.newaxis,:]\n",
    "                    category_all = y[np.newaxis,:]\n",
    "                    ft = False\n",
    "                else:\n",
    "                    image_all = np.append(image_all, image_merged[np.newaxis, :], axis=0)\n",
    "                    category_all = np.append(category_all, y[np.newaxis, :], axis=0)\n",
    "            z.close()\n",
    "#            print(sys.getsizeof(image_all), sys.getsizeof(category_all), image_all.shape)\n",
    "            yield image_all, category_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc1 = 'models/11_l_checkpoint.h5'\n",
    "direc1_cross_entropy = 'models/11_lf_checkpoint.h5'\n",
    "direc2 = 'models/11_l_eval.txt'\n",
    "direc_out = 'submissions/submission11_l.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=0)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              mode='auto', period=1)\n",
    "\n",
    "blocks=[2,4,8,4]\n",
    "n=32\n",
    "k=32\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(128,128,4,))\n",
    "\n",
    "\n",
    "x = Conv2D(n, (1,1), kernel_initializer='he_normal')(xin)\n",
    "x, n = dense_block(x, n, k, blocks[0])\n",
    "gap1 = GlobalAveragePooling2D()(x)\n",
    "x, n = transition_layer(x, n)\n",
    "\n",
    "x, n = dense_block(x, n, k, blocks[1])\n",
    "gap2 = GlobalAveragePooling2D()(x)\n",
    "x, n = transition_layer(x, n)\n",
    "\n",
    "x, n = dense_block(x, n, k, blocks[2])\n",
    "gap3 = GlobalAveragePooling2D()(x)\n",
    "x, n = transition_layer(x, n)\n",
    "\n",
    "x, n = dense_block(x, n, k, blocks[3])\n",
    "gap4 = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Concatenate()([gap1, gap2, gap3, gap4])\n",
    "x = Dense(256)(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256)(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=xin, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 4224        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 128 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 128 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 32) 36896       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 128 8320        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 128 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128, 128, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 32) 36896       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 48) 3120        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 64, 64, 48)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 48)   192         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 64, 64, 48)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  6272        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 32)   36896       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 80)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 80)   320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 64, 64, 80)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 128)  10368       leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 64, 64, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 32)   36896       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 80)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 80)   320         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 64, 64, 80)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 128)  10368       leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 32)   36896       leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 80)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 80)   320         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 64, 64, 80)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  10368       leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   36896       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 80)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 88)   7128        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 32, 32, 88)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 88)   352         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 32, 32, 88)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  11392       leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 120)  480         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 120)  480         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 120)  480         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 120)  480         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 120)  480         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 120)  480         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 32, 32, 120)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 128)  15488       leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 32)   36896       leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 120)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 172)  20812       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 16, 16, 172)  0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 172)  688         average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 16, 16, 172)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 128)  22144       leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 32)   36896       leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 204)  0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 204)  816         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 16, 16, 204)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 128)  26240       leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 32)   36896       leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 204)  0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 204)  816         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 16, 16, 204)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 128)  26240       leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 32)   36896       leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 204)  0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 204)  816         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 16, 16, 204)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 128)  26240       leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 128)  512         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 32)   36896       leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 204)  0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 80)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 120)          0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 204)          0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 468)          0           global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "                                                                 global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          120064      concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 28)           7196        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,176,592\n",
      "Trainable params: 1,167,792\n",
      "Non-trainable params: 8,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = keras.models.load_model(direc1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  48/1923 [..............................] - ETA: 36:54 - loss: 0.1100 - acc: 0.9608 - f1: 0.2031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4fa78389f58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_es\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     max_queue_size=10)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec1_cross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_gen = generateDatav2aug('data/train.zip', x_train_mini, y_train_mini, batch_size=batch_size)\n",
    "valid_gen = generateDatav2('data/train.zip', x_test_mini, y_test_mini, batch_size=batch_size)\n",
    "\n",
    "#model.load_weights(direc1_cross_entropy)\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=math.ceil(len(x_train_mini) / batch_size), \n",
    "                    epochs=1000,  \n",
    "                    validation_data = valid_gen,\n",
    "                    validation_steps = math.ceil(len(x_test_mini) / batch_size),\n",
    "                    callbacks=[cb_es, check_point], \n",
    "                    verbose=1,\n",
    "                    max_queue_size=10)\n",
    "\n",
    "model.save_weights(direc1_cross_entropy)\n",
    "\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "model.load_weights(direc1_cross_entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "962/962 [==============================] - 1056s 1s/step - loss: 0.1546 - acc: 0.9487 - f1: 0.1329 - val_loss: 0.1485 - val_acc: 0.9482 - val_f1: 0.1430\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14853, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 2/1000\n",
      "962/962 [==============================] - 1057s 1s/step - loss: 0.1489 - acc: 0.9498 - f1: 0.1454 - val_loss: 0.1432 - val_acc: 0.9520 - val_f1: 0.1500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14853 to 0.14325, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 3/1000\n",
      "962/962 [==============================] - 1070s 1s/step - loss: 0.1445 - acc: 0.9507 - f1: 0.1564 - val_loss: 0.1404 - val_acc: 0.9517 - val_f1: 0.1655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14325 to 0.14036, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 4/1000\n",
      "962/962 [==============================] - 1084s 1s/step - loss: 0.1391 - acc: 0.9519 - f1: 0.1676 - val_loss: 0.1346 - val_acc: 0.9522 - val_f1: 0.1727\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14036 to 0.13456, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 5/1000\n",
      "962/962 [==============================] - 1083s 1s/step - loss: 0.1339 - acc: 0.9534 - f1: 0.1780 - val_loss: 0.1356 - val_acc: 0.9538 - val_f1: 0.1721\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13456\n",
      "Epoch 6/1000\n",
      "962/962 [==============================] - 1087s 1s/step - loss: 0.1290 - acc: 0.9549 - f1: 0.1894 - val_loss: 0.1291 - val_acc: 0.9550 - val_f1: 0.2012\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13456 to 0.12913, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 7/1000\n",
      "962/962 [==============================] - 1083s 1s/step - loss: 0.1255 - acc: 0.9563 - f1: 0.1973 - val_loss: 0.1277 - val_acc: 0.9556 - val_f1: 0.2006\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12913 to 0.12773, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 8/1000\n",
      "962/962 [==============================] - 1081s 1s/step - loss: 0.1223 - acc: 0.9574 - f1: 0.2040 - val_loss: 0.1260 - val_acc: 0.9552 - val_f1: 0.2238\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12773 to 0.12604, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 9/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.1197 - acc: 0.9581 - f1: 0.2110 - val_loss: 0.1225 - val_acc: 0.9560 - val_f1: 0.2180\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12604 to 0.12248, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 10/1000\n",
      "962/962 [==============================] - 1087s 1s/step - loss: 0.1177 - acc: 0.9586 - f1: 0.2160 - val_loss: 0.1172 - val_acc: 0.9576 - val_f1: 0.2092\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12248 to 0.11723, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 11/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.1155 - acc: 0.9593 - f1: 0.2203 - val_loss: 0.1180 - val_acc: 0.9574 - val_f1: 0.2254\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11723\n",
      "Epoch 12/1000\n",
      "962/962 [==============================] - 1090s 1s/step - loss: 0.1143 - acc: 0.9596 - f1: 0.2236 - val_loss: 0.1158 - val_acc: 0.9577 - val_f1: 0.2341\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11723 to 0.11581, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 13/1000\n",
      "962/962 [==============================] - 1091s 1s/step - loss: 0.1129 - acc: 0.9600 - f1: 0.2281 - val_loss: 0.1142 - val_acc: 0.9587 - val_f1: 0.2466\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11581 to 0.11416, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 14/1000\n",
      "962/962 [==============================] - 1096s 1s/step - loss: 0.1111 - acc: 0.9605 - f1: 0.2326 - val_loss: 0.1107 - val_acc: 0.9599 - val_f1: 0.2404\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11416 to 0.11074, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 15/1000\n",
      "962/962 [==============================] - 1103s 1s/step - loss: 0.1102 - acc: 0.9610 - f1: 0.2341 - val_loss: 0.1132 - val_acc: 0.9610 - val_f1: 0.2361\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11074\n",
      "Epoch 16/1000\n",
      "962/962 [==============================] - 1102s 1s/step - loss: 0.1091 - acc: 0.9613 - f1: 0.2363 - val_loss: 0.1176 - val_acc: 0.9575 - val_f1: 0.2395\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11074\n",
      "Epoch 17/1000\n",
      "962/962 [==============================] - 1094s 1s/step - loss: 0.1080 - acc: 0.9615 - f1: 0.2397 - val_loss: 0.1071 - val_acc: 0.9624 - val_f1: 0.2457\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11074 to 0.10706, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 18/1000\n",
      "962/962 [==============================] - 1088s 1s/step - loss: 0.1074 - acc: 0.9618 - f1: 0.2403 - val_loss: 0.1073 - val_acc: 0.9622 - val_f1: 0.2554\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10706\n",
      "Epoch 19/1000\n",
      "962/962 [==============================] - 1084s 1s/step - loss: 0.1061 - acc: 0.9623 - f1: 0.2441 - val_loss: 0.1096 - val_acc: 0.9619 - val_f1: 0.2430\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10706\n",
      "Epoch 20/1000\n",
      "962/962 [==============================] - 1084s 1s/step - loss: 0.1052 - acc: 0.9625 - f1: 0.2453 - val_loss: 0.1010 - val_acc: 0.9643 - val_f1: 0.2617\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10706 to 0.10100, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 21/1000\n",
      "962/962 [==============================] - 1083s 1s/step - loss: 0.1047 - acc: 0.9626 - f1: 0.2485 - val_loss: 0.1041 - val_acc: 0.9637 - val_f1: 0.2598\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10100\n",
      "Epoch 22/1000\n",
      "962/962 [==============================] - 1087s 1s/step - loss: 0.1036 - acc: 0.9631 - f1: 0.2519 - val_loss: 0.1033 - val_acc: 0.9646 - val_f1: 0.2657\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10100\n",
      "Epoch 23/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.1029 - acc: 0.9633 - f1: 0.2512 - val_loss: 0.1045 - val_acc: 0.9647 - val_f1: 0.2673\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10100\n",
      "Epoch 24/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.1025 - acc: 0.9633 - f1: 0.2527 - val_loss: 0.1025 - val_acc: 0.9630 - val_f1: 0.2618\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10100\n",
      "Epoch 25/1000\n",
      "962/962 [==============================] - 1087s 1s/step - loss: 0.1016 - acc: 0.9635 - f1: 0.2567 - val_loss: 0.0998 - val_acc: 0.9659 - val_f1: 0.2728\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10100 to 0.09980, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 26/1000\n",
      "962/962 [==============================] - 1081s 1s/step - loss: 0.1006 - acc: 0.9638 - f1: 0.2588 - val_loss: 0.1113 - val_acc: 0.9614 - val_f1: 0.2570\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09980\n",
      "Epoch 27/1000\n",
      "962/962 [==============================] - 1083s 1s/step - loss: 0.1002 - acc: 0.9640 - f1: 0.2593 - val_loss: 0.1052 - val_acc: 0.9626 - val_f1: 0.2641\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09980\n",
      "Epoch 28/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.1000 - acc: 0.9641 - f1: 0.2592 - val_loss: 0.0996 - val_acc: 0.9642 - val_f1: 0.2642\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09980 to 0.09960, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 29/1000\n",
      "962/962 [==============================] - 1083s 1s/step - loss: 0.0993 - acc: 0.9643 - f1: 0.2616 - val_loss: 0.1038 - val_acc: 0.9639 - val_f1: 0.2569\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09960\n",
      "Epoch 30/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.0987 - acc: 0.9643 - f1: 0.2626 - val_loss: 0.0980 - val_acc: 0.9660 - val_f1: 0.2658\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09960 to 0.09798, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 31/1000\n",
      "962/962 [==============================] - 1087s 1s/step - loss: 0.0981 - acc: 0.9647 - f1: 0.2634 - val_loss: 0.1068 - val_acc: 0.9621 - val_f1: 0.2694\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09798\n",
      "Epoch 32/1000\n",
      "962/962 [==============================] - 1088s 1s/step - loss: 0.0975 - acc: 0.9648 - f1: 0.2667 - val_loss: 0.1034 - val_acc: 0.9630 - val_f1: 0.2660\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09798\n",
      "Epoch 33/1000\n",
      "962/962 [==============================] - 1085s 1s/step - loss: 0.0971 - acc: 0.9649 - f1: 0.2670 - val_loss: 0.1010 - val_acc: 0.9628 - val_f1: 0.2753\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09798\n",
      "Epoch 34/1000\n",
      "962/962 [==============================] - 1081s 1s/step - loss: 0.0967 - acc: 0.9651 - f1: 0.2671 - val_loss: 0.1003 - val_acc: 0.9645 - val_f1: 0.2607\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09798\n",
      "Epoch 35/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 1082s 1s/step - loss: 0.0957 - acc: 0.9653 - f1: 0.2709 - val_loss: 0.0973 - val_acc: 0.9659 - val_f1: 0.2758\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09798 to 0.09729, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 36/1000\n",
      "962/962 [==============================] - 1081s 1s/step - loss: 0.0956 - acc: 0.9654 - f1: 0.2710 - val_loss: 0.0996 - val_acc: 0.9639 - val_f1: 0.2635\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09729\n",
      "Epoch 37/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.0951 - acc: 0.9657 - f1: 0.2721 - val_loss: 0.1012 - val_acc: 0.9646 - val_f1: 0.2648\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09729\n",
      "Epoch 38/1000\n",
      "962/962 [==============================] - 1082s 1s/step - loss: 0.0946 - acc: 0.9656 - f1: 0.2727 - val_loss: 0.1033 - val_acc: 0.9641 - val_f1: 0.2465\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09729\n",
      "Epoch 39/1000\n",
      "962/962 [==============================] - 1086s 1s/step - loss: 0.0940 - acc: 0.9658 - f1: 0.2765 - val_loss: 0.1005 - val_acc: 0.9654 - val_f1: 0.2646\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09729\n",
      "Epoch 40/1000\n",
      "962/962 [==============================] - 1081s 1s/step - loss: 0.0941 - acc: 0.9660 - f1: 0.2750 - val_loss: 0.1030 - val_acc: 0.9644 - val_f1: 0.2634\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09729\n",
      "Epoch 41/1000\n",
      "962/962 [==============================] - 1083s 1s/step - loss: 0.0938 - acc: 0.9660 - f1: 0.2755 - val_loss: 0.1001 - val_acc: 0.9638 - val_f1: 0.2686\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09729\n",
      "Epoch 42/1000\n",
      "962/962 [==============================] - 1080s 1s/step - loss: 0.0932 - acc: 0.9663 - f1: 0.2768 - val_loss: 0.1032 - val_acc: 0.9637 - val_f1: 0.2535\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09729\n",
      "Epoch 43/1000\n",
      "962/962 [==============================] - 1084s 1s/step - loss: 0.0925 - acc: 0.9664 - f1: 0.2775 - val_loss: 0.1015 - val_acc: 0.9620 - val_f1: 0.2723\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09729\n",
      "Epoch 44/1000\n",
      "962/962 [==============================] - 1075s 1s/step - loss: 0.0921 - acc: 0.9665 - f1: 0.2785 - val_loss: 0.1023 - val_acc: 0.9633 - val_f1: 0.2747\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09729\n",
      "Epoch 45/1000\n",
      "962/962 [==============================] - 1078s 1s/step - loss: 0.0920 - acc: 0.9665 - f1: 0.2799 - val_loss: 0.1046 - val_acc: 0.9629 - val_f1: 0.2558\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09729\n",
      "Epoch 46/1000\n",
      "962/962 [==============================] - 1077s 1s/step - loss: 0.0916 - acc: 0.9665 - f1: 0.2809 - val_loss: 0.1034 - val_acc: 0.9623 - val_f1: 0.2648\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09729\n",
      "Epoch 47/1000\n",
      "962/962 [==============================] - 1078s 1s/step - loss: 0.0912 - acc: 0.9670 - f1: 0.2817 - val_loss: 0.1128 - val_acc: 0.9600 - val_f1: 0.2522\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09729\n",
      "Epoch 48/1000\n",
      "962/962 [==============================] - 1081s 1s/step - loss: 0.0908 - acc: 0.9668 - f1: 0.2836 - val_loss: 0.0972 - val_acc: 0.9668 - val_f1: 0.2718\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09729 to 0.09720, saving model to models/11_l_checkpoint.h5\n",
      "Epoch 49/1000\n",
      "962/962 [==============================] - 1085s 1s/step - loss: 0.0902 - acc: 0.9671 - f1: 0.2842 - val_loss: 0.1004 - val_acc: 0.9634 - val_f1: 0.2705\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09720\n",
      "Epoch 50/1000\n",
      "962/962 [==============================] - 1082s 1s/step - loss: 0.0899 - acc: 0.9672 - f1: 0.2846 - val_loss: 0.1112 - val_acc: 0.9613 - val_f1: 0.2675\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09720\n",
      "Epoch 51/1000\n",
      "962/962 [==============================] - 1103s 1s/step - loss: 0.0895 - acc: 0.9673 - f1: 0.2867 - val_loss: 0.1035 - val_acc: 0.9637 - val_f1: 0.2700\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09720\n",
      "Epoch 52/1000\n",
      "962/962 [==============================] - 1104s 1s/step - loss: 0.0891 - acc: 0.9675 - f1: 0.2873 - val_loss: 0.1044 - val_acc: 0.9623 - val_f1: 0.2709\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09720\n",
      "Epoch 53/1000\n",
      "962/962 [==============================] - 1073s 1s/step - loss: 0.0890 - acc: 0.9676 - f1: 0.2872 - val_loss: 0.1042 - val_acc: 0.9634 - val_f1: 0.2669\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09720\n",
      "Epoch 54/1000\n",
      "962/962 [==============================] - 1075s 1s/step - loss: 0.0885 - acc: 0.9677 - f1: 0.2896 - val_loss: 0.1081 - val_acc: 0.9615 - val_f1: 0.2665\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09720\n",
      "Epoch 55/1000\n",
      " 16/962 [..............................] - ETA: 23:49 - loss: 0.0886 - acc: 0.9662 - f1: 0.2849"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "\n",
    "train_gen = generateDataaug('data/train.zip', x_train_mini, y_train_mini, batch_size=batch_size)\n",
    "valid_gen = generateData('data/train.zip', x_test_mini, y_test_mini, batch_size=batch_size)\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=math.ceil(len(x_train_mini) / batch_size), \n",
    "                    epochs=1000,  \n",
    "                    validation_data = valid_gen,\n",
    "                    validation_steps = math.ceil(len(x_test_mini) / batch_size),\n",
    "                    callbacks=[cb_es, check_point], \n",
    "                    verbose=1,\n",
    "                    max_queue_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008af0-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000a892-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006faa6-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008baca-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cce7e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  Predicted\n",
       "0  00008af0-bad0-11e8-b2b8-ac1f6b6435d0          0\n",
       "1  0000a892-bacf-11e8-b2b8-ac1f6b6435d0          0\n",
       "2  0006faa6-bac7-11e8-b2b7-ac1f6b6435d0          0\n",
       "3  0008baca-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "4  000cce7e-bad4-11e8-b2b8-ac1f6b6435d0          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict from submission.csv\n",
    "data_sub = pd.read_csv(\"data/sample_submission.csv\", engine='python')\n",
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Data_prediction(zippath, xs, batch_size=128, imsize=128):\n",
    "    n_batches = math.ceil(len(xs) / batch_size)\n",
    "    while True:   \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            ft = True\n",
    "            z = zipfile.ZipFile(zippath)\n",
    "            for dat in xs[start:end]:\n",
    "                imgname_red = dat+'_red.png'\n",
    "                img_red = np.array(Image.open( io.BytesIO(z.read(imgname_red)) ).resize((imsize, imsize))).astype('float32')\n",
    "                imgname_blue = dat+'_blue.png'\n",
    "                img_blue = np.array(Image.open( io.BytesIO(z.read(imgname_blue)) ).resize((imsize, imsize))).astype('float32')\n",
    "                imgname_green = dat+'_green.png'\n",
    "                img_green = np.array(Image.open( io.BytesIO(z.read(imgname_green)) ).resize((imsize, imsize))).astype('float32')\n",
    "                imgname_yellow = dat+'_yellow.png'\n",
    "                img_yellow = np.array(Image.open( io.BytesIO(z.read(imgname_yellow)) ).resize((imsize, imsize))).astype('float32')\n",
    "                #define as numpy 512 x 512 x 4\n",
    "                image_merged = np.append(img_red[:,:,np.newaxis],img_blue[:,:,np.newaxis], axis=-1)\n",
    "                image_merged = np.append(image_merged,img_green[:,:,np.newaxis], axis=-1)\n",
    "                image_merged = np.append(image_merged,img_yellow[:,:,np.newaxis], axis=-1)\n",
    "                image_merged = image_merged/255 #standardize\n",
    "                #print(sys.getsizeof(image_merged))\n",
    "                #append as one numpy (for saving)\n",
    "                if ft:\n",
    "                    image_all = image_merged[np.newaxis,:]\n",
    "                    ft = False\n",
    "                else:\n",
    "                    image_all = np.append(image_all, image_merged[np.newaxis, :], axis=0)\n",
    "            z.close()\n",
    "            #print(sys.getsizeof(image_all), sys.getsizeof(category_all), image_all.shape)\n",
    "            yield image_all\n",
    "            \n",
    "def gen_Data_predictionv2(zippath, xs, batch_size=128):\n",
    "    n_batches = math.ceil(len(xs) / batch_size)\n",
    "    while True:   \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            ft = True\n",
    "            z = zipfile.ZipFile(zippath)\n",
    "            for dat in xs[start:end]:\n",
    "                imgname_green = dat+'_green.png'\n",
    "                img_green = np.array(Image.open( io.BytesIO(z.read(imgname_green)) )).astype('float32')\n",
    "                img_green = img_green/np.max(img_green)\n",
    "                img_green = img_green.reshape(1, 512, 512, 1)\n",
    "                imgname_red = dat+'_red.png'\n",
    "                img_red = np.array(Image.open( io.BytesIO(z.read(imgname_red)) )).astype('float32')\n",
    "                img_red = img_red/np.max(img_red)\n",
    "                img_red = img_red.reshape(1, 512, 512, 1)\n",
    "                imgname_blue = dat+'_blue.png'\n",
    "                img_blue = np.array(Image.open( io.BytesIO(z.read(imgname_blue)) )).astype('float32')\n",
    "                img_blue = img_blue/np.max(img_blue)\n",
    "                img_blue = img_blue.reshape(1, 512, 512, 1)\n",
    "                imgname_yellow = dat+'_yellow.png'\n",
    "                img_yellow = np.array(Image.open( io.BytesIO(z.read(imgname_yellow)) )).astype('float32')\n",
    "                img_yellow = img_yellow/np.max(img_yellow)\n",
    "                img_yellow = img_yellow.reshape(1, 512, 512, 1)\n",
    "                #append as one numpy (for saving)\n",
    "                if ft:\n",
    "                    img_green_batch = img_green\n",
    "                    img_red_batch = img_red\n",
    "                    img_blue_batch = img_blue\n",
    "                    img_yellow_batch = img_yellow\n",
    "                    ft = False\n",
    "                else:\n",
    "                    img_green_batch = np.append(img_green_batch, img_green, axis=0)\n",
    "                    img_red_batch = np.append(img_red_batch, img_red, axis=0)\n",
    "                    img_blue_batch = np.append(img_blue_batch, img_blue, axis=0)\n",
    "                    img_yellow_batch = np.append(img_yellow_batch, img_yellow, axis=0)\n",
    "            z.close()\n",
    "#            print(sys.getsizeof(image_all), sys.getsizeof(category_all), image_all.shape)\n",
    "            yield [img_green_batch, img_red_batch, img_blue_batch , img_yellow_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc1 = 'models/11_l_checkpoint.h5'\n",
    "direc1_cross_entropy = 'models/11_final_checkpoint.h5'\n",
    "direc2 = 'models/11_l_eval.txt'\n",
    "direc_out = 'submissions/submission11_l.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=123)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "ch = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=[xing, xinr, xinb, xiny], outputs=y)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 585s 3s/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "x_test = data_sub['Id']\n",
    "\n",
    "pred_gen = gen_Data_predictionv2('data/test.zip', x_test, batch_size=batch_size)\n",
    "\n",
    "pred_y = model.predict_generator(pred_gen, steps=math.ceil(len(x_test) / batch_size), verbose=1)\n",
    "\n",
    "i = 0\n",
    "data_sub['Predicted'] = data_sub['Predicted'].astype(object)\n",
    "for y_sub in pred_y:\n",
    "    ft=True\n",
    "    for ar in np.where(y_sub>0.1)[0]:\n",
    "        #print(i,\" \",np.where(y_sub>0.5)[0])\n",
    "        if ft:\n",
    "            data_sub['Predicted'][i] = str(ar)\n",
    "            ft = False\n",
    "        else:\n",
    "            data_sub['Predicted'][i] = data_sub['Predicted'][i] +' '+ str(ar)\n",
    "    if ft:\n",
    "        data_sub['Predicted'][i] = np.where(y_sub==np.max(y_sub))[0][0]\n",
    "    i+=1    \n",
    "\n",
    "data_sub.to_csv(direc_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "data_sub['Predicted'] = data_sub['Predicted'].astype(object)\n",
    "for y_sub in pred_y:\n",
    "    ft=True\n",
    "    for ar in np.where(y_sub>0.3)[0]:\n",
    "        #print(i,\" \",np.where(y_sub>0.3)[0])\n",
    "        if ft:\n",
    "            data_sub['Predicted'][i] = str(ar)\n",
    "            ft = False\n",
    "        else:\n",
    "            data_sub['Predicted'][i] = data_sub['Predicted'][i] +' '+ str(ar)\n",
    "    if ft:\n",
    "        data_sub['Predicted'][i] = np.where(y_sub==np.max(y_sub))[0][0]\n",
    "    i+=1    \n",
    "\n",
    "data_sub.to_csv(direc_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sub = pred_y[1]\n",
    "np.where(y_sub>0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.70158070e-01,   6.25024885e-02,   4.75552678e-02,\n",
       "         3.34178545e-02,   5.42743802e-02,   1.65147826e-01,\n",
       "         8.96948669e-03,   1.80736054e-02,   3.88039371e-05,\n",
       "         1.76359626e-05,   2.26985912e-06,   1.03038698e-02,\n",
       "         5.95179107e-03,   8.25559534e-03,   2.11130977e-02,\n",
       "         5.14525105e-04,   2.69827731e-02,   1.14835799e-02,\n",
       "         2.89440583e-02,   7.02944696e-02,   1.48548046e-03,\n",
       "         9.48961452e-02,   1.29110264e-02,   1.44278649e-02,\n",
       "         1.74770271e-03,   2.56729752e-01,   2.10440643e-02,\n",
       "         5.18633096e-05], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008af0-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>2 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000a892-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006faa6-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0 14 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008baca-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cce7e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00109f6a-bac8-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>001765de-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0018641a-bac9-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00200f22-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0026f154-bac6-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>002729d2-bace-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>002c1a7c-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>003170fa-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0031820a-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00407c16-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>005ce2ea-bacc-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00631ec8-bad9-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>2 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00673f64-bad2-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0070171c-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>21 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>007290b6-bad8-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0075ee26-bacb-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00763d66-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>007eca68-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>008ab0b8-bad5-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>008e8c3e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>009131e6-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0094159e-bad1-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0097a5c2-bac9-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>009d32e4-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00a4925a-bad8-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11672</th>\n",
       "      <td>ff2cd716-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>ff2d3abc-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>ff38cec0-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11675</th>\n",
       "      <td>ff489096-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>ff49d834-bad2-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11677</th>\n",
       "      <td>ff4aa9a4-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0 21 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11678</th>\n",
       "      <td>ff5464dc-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 21 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>ff56b30a-bace-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11680</th>\n",
       "      <td>ff581dac-bac5-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>ff5ee18c-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>ff67db02-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>ff7cc20c-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ff7dc452-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>ff8346fa-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11686</th>\n",
       "      <td>ff8e580c-bad1-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11687</th>\n",
       "      <td>ff9659a0-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11688</th>\n",
       "      <td>ff9bdfd8-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 21 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11689</th>\n",
       "      <td>ff9ccd2a-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11690</th>\n",
       "      <td>ffa6bc7e-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11691</th>\n",
       "      <td>ffad96dc-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11692</th>\n",
       "      <td>ffb91448-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11693</th>\n",
       "      <td>ffd677a0-bada-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11694</th>\n",
       "      <td>ffd72db2-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11695</th>\n",
       "      <td>ffd83fa4-bacb-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>ffd91122-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11697</th>\n",
       "      <td>ffdfb96a-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11698</th>\n",
       "      <td>ffdfc590-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>ffecb8a4-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11700</th>\n",
       "      <td>fff03816-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11701</th>\n",
       "      <td>fffe6f9c-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11702 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Id Predicted\n",
       "0      00008af0-bad0-11e8-b2b8-ac1f6b6435d0       2 3\n",
       "1      0000a892-bacf-11e8-b2b8-ac1f6b6435d0         0\n",
       "2      0006faa6-bac7-11e8-b2b7-ac1f6b6435d0   0 14 25\n",
       "3      0008baca-bad7-11e8-b2b9-ac1f6b6435d0      0 25\n",
       "4      000cce7e-bad4-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "5      00109f6a-bac8-11e8-b2b7-ac1f6b6435d0         0\n",
       "6      001765de-bacd-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "7      0018641a-bac9-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "8      00200f22-bad7-11e8-b2b9-ac1f6b6435d0         0\n",
       "9      0026f154-bac6-11e8-b2b7-ac1f6b6435d0      0 25\n",
       "10     002729d2-bace-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "11     002c1a7c-bad4-11e8-b2b8-ac1f6b6435d0         0\n",
       "12     003170fa-bacd-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "13     0031820a-baca-11e8-b2b8-ac1f6b6435d0         0\n",
       "14     00407c16-bad3-11e8-b2b8-ac1f6b6435d0         0\n",
       "15     005ce2ea-bacc-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "16     00631ec8-bad9-11e8-b2b9-ac1f6b6435d0       2 3\n",
       "17     00673f64-bad2-11e8-b2b8-ac1f6b6435d0         0\n",
       "18     0070171c-bad0-11e8-b2b8-ac1f6b6435d0     21 25\n",
       "19     007290b6-bad8-11e8-b2b9-ac1f6b6435d0         0\n",
       "20     0075ee26-bacb-11e8-b2b8-ac1f6b6435d0        25\n",
       "21     00763d66-bacd-11e8-b2b8-ac1f6b6435d0         0\n",
       "22     007eca68-bac7-11e8-b2b7-ac1f6b6435d0         0\n",
       "23     008ab0b8-bad5-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "24     008e8c3e-bad4-11e8-b2b8-ac1f6b6435d0         0\n",
       "25     009131e6-baca-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "26     0094159e-bad1-11e8-b2b8-ac1f6b6435d0      0 21\n",
       "27     0097a5c2-bac9-11e8-b2b8-ac1f6b6435d0         0\n",
       "28     009d32e4-bad3-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "29     00a4925a-bad8-11e8-b2b9-ac1f6b6435d0         0\n",
       "...                                     ...       ...\n",
       "11672  ff2cd716-bad7-11e8-b2b9-ac1f6b6435d0         0\n",
       "11673  ff2d3abc-bad7-11e8-b2b9-ac1f6b6435d0      0 25\n",
       "11674  ff38cec0-bad4-11e8-b2b8-ac1f6b6435d0         0\n",
       "11675  ff489096-bad7-11e8-b2b9-ac1f6b6435d0      0 25\n",
       "11676  ff49d834-bad2-11e8-b2b8-ac1f6b6435d0         0\n",
       "11677  ff4aa9a4-bac7-11e8-b2b7-ac1f6b6435d0   0 21 25\n",
       "11678  ff5464dc-baca-11e8-b2b8-ac1f6b6435d0   0 21 22\n",
       "11679  ff56b30a-bace-11e8-b2b8-ac1f6b6435d0         0\n",
       "11680  ff581dac-bac5-11e8-b2b7-ac1f6b6435d0         0\n",
       "11681  ff5ee18c-baca-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "11682  ff67db02-baca-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "11683  ff7cc20c-bad3-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "11684  ff7dc452-bad5-11e8-b2b9-ac1f6b6435d0         0\n",
       "11685  ff8346fa-bad7-11e8-b2b9-ac1f6b6435d0        24\n",
       "11686  ff8e580c-bad1-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "11687  ff9659a0-bacf-11e8-b2b8-ac1f6b6435d0         0\n",
       "11688  ff9bdfd8-baca-11e8-b2b8-ac1f6b6435d0   0 21 25\n",
       "11689  ff9ccd2a-bad0-11e8-b2b8-ac1f6b6435d0        21\n",
       "11690  ffa6bc7e-bad3-11e8-b2b8-ac1f6b6435d0        25\n",
       "11691  ffad96dc-bad5-11e8-b2b9-ac1f6b6435d0         0\n",
       "11692  ffb91448-bac7-11e8-b2b7-ac1f6b6435d0         0\n",
       "11693  ffd677a0-bada-11e8-b2b9-ac1f6b6435d0         0\n",
       "11694  ffd72db2-bad5-11e8-b2b9-ac1f6b6435d0         0\n",
       "11695  ffd83fa4-bacb-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "11696  ffd91122-bad0-11e8-b2b8-ac1f6b6435d0         0\n",
       "11697  ffdfb96a-bad4-11e8-b2b8-ac1f6b6435d0        19\n",
       "11698  ffdfc590-bad4-11e8-b2b8-ac1f6b6435d0         0\n",
       "11699  ffecb8a4-bad4-11e8-b2b8-ac1f6b6435d0         0\n",
       "11700  fff03816-bad5-11e8-b2b9-ac1f6b6435d0         0\n",
       "11701  fffe6f9c-bacd-11e8-b2b8-ac1f6b6435d0      0 25\n",
       "\n",
       "[11702 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "direc1 = 'models/01_checkpoint.h5'\n",
    "direc2 = 'models/01_eval.txt'\n",
    "direc_out = 'submissions/submission01_x.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2\n",
    ")\n",
    "datagen.fit(x_train_mini)\n",
    "\n",
    "activation = 'relu'\n",
    "batch_size = 16\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "epochs = 100\n",
    "blocks=[2,4,8,4]\n",
    "n=32\n",
    "k=32\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(512,512,4,))\n",
    "\n",
    "x = Conv2D(4, (3,3), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(xin)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(8, (1,1), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(8, (3,3), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(16, (1,1), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3,3), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "'''\n",
    "x = Conv2D(16, (1,1), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3,3), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "'''\n",
    "\n",
    "x = Conv2D(n, (1,1), kernel_initializer='he_normal')(x)\n",
    "for i in range(len(blocks)):\n",
    "    #Transition\n",
    "    if i != 0:\n",
    "        x, n = transition_layer(x, n)\n",
    "    #DenseBlock\n",
    "    x, n = dense_block(x, n, k, blocks[i])\n",
    "\n",
    "\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "direc1 = 'models/03_checkpoint.h5'\n",
    "direc2 = 'models/03_eval.txt'\n",
    "direc_out = 'submissions/submission03.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "batch_size = 4\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "epochs = 100\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(512,512,4,))\n",
    "\n",
    "x = resblock2(xin, filters=8, kernel_size=(3, 3), activation='relu')\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = resblock2(x, filters=16, kernel_size=(3, 3), activation='relu')\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = resblock2(x, filters=32, kernel_size=(3, 3), activation='relu')\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = resblock2(x, filters=64, kernel_size=(3, 3), activation='relu')\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = resblock2(x, filters=128, kernel_size=(3, 3), activation='relu')\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled!\n"
     ]
    }
   ],
   "source": [
    "direc1 = 'models/05_checkpoint.h5'\n",
    "direc2 = 'models/05_eval.txt'\n",
    "direc_out = 'submissions/submission05_x.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "batch_size = 8\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "epochs = 100\n",
    "blocks=[2,4,8,4]\n",
    "n=32\n",
    "k=32\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(512,512,4,))\n",
    "\n",
    "x = Conv2D(8, (5,5), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal', name='conv1')(xin)\n",
    "x = BatchNormalization()(x)\n",
    "xt = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(16, (3,3), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal', name='conv2')(x)\n",
    "x = BatchNormalization()(xt)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(32, (3,3), padding='same', activation=activation, \n",
    "           kernel_initializer='he_normal', name='conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "#x = Conv2D(16, (1,1), padding='same', activation=activation, \n",
    "#           kernel_initializer='he_normal')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Conv2D(16, (3,3), padding='same', activation=activation, \n",
    "#           kernel_initializer='he_normal')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "\n",
    "x = Conv2D(n, (1,1), kernel_initializer='he_normal')(x)\n",
    "for i in range(len(blocks)):\n",
    "    #Transition\n",
    "    if i != 0:\n",
    "        x, n = transition_layer(x, n)\n",
    "    #DenseBlock\n",
    "    x, n = dense_block(x, n, k, blocks[i])\n",
    "\n",
    "\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[f1_loss], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "model =  keras.models.load_model(direc1, custom_objects={'f1_loss': f1_loss, 'f1': f1})\n",
    "K.set_value(model.optimizer.lr, 0.0005)\n",
    "\n",
    "print('compiled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled!\n"
     ]
    }
   ],
   "source": [
    "direc1 = 'models/06_checkpoint.h5'\n",
    "direc2 = 'models/06_eval.txt'\n",
    "direc_out = 'submissions/submission06.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "epochs = 100\n",
    "\n",
    "#network\n",
    "encoder = keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(512,512,3), pooling='avg')\n",
    "\n",
    "xin = Input(shape=(512,512,4,))\n",
    "x = BatchNormalization()(xin)\n",
    "x = Conv2D(3, kernel_size=(1,1), activation='relu')(xin)\n",
    "x = BatchNormalization()(x)\n",
    "x = encoder(x)\n",
    "#x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[f1_loss], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "#model =  keras.models.load_model(direc1, custom_objects={'f1_loss': f1_loss, 'f1': f1})\n",
    "#K.set_value(model.optimizer.lr, 0.0005)\n",
    "\n",
    "print('compiled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "direc1 = 'models/07_checkpoint.h5'\n",
    "direc2 = 'models/07_eval.txt'\n",
    "direc_out = 'submissions/submission07.csv'\n",
    "\n",
    "load_weight = False\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='auto', period=1)\n",
    "epochs = 1000\n",
    "blocks=[2,4,8,4]\n",
    "n=32\n",
    "k=32\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(512,512,4,))\n",
    "\n",
    "x = Conv2D(8, (3,3), strides=(2,2), padding='same',  \n",
    "           kernel_initializer='he_normal')(xin) #512x512x4 -> 256x256x8\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = Conv2D(16, (3,3), strides=(2,2), padding='same', \n",
    "           kernel_initializer='he_normal')(x) # 256x256x8 -> 128x128x16\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = Conv2D(n, (1,1), kernel_initializer='he_normal')(x)\n",
    "for i in range(len(blocks)):\n",
    "    #Transition\n",
    "    if i != 0:\n",
    "        x, n = transition_layer(x, n)\n",
    "    #DenseBlock\n",
    "    x, n = dense_block(x, n, k, blocks[i])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "if load_weight:\n",
    "    model.load_weights(direc1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc1 = 'models/08_checkpoint.h5'\n",
    "direc2 = 'models/08_eval.txt'\n",
    "direc_out = 'submissions/submission08.csv'\n",
    "\n",
    "load_weight = False\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='auto', period=1)\n",
    "epochs = 1000\n",
    "blocks=[2,4,8,4]\n",
    "n=32\n",
    "k=32\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(512,512,4,))\n",
    "\n",
    "x = Conv2D(8, (3,3), strides=(2,2), padding='same',  \n",
    "           kernel_initializer='he_normal')(xin) #512x512x4 -> 256x256x8\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Conv2D(16, (3,3), strides=(2,2), padding='same', \n",
    "           kernel_initializer='he_normal')(x) # 256x256x8 -> 128x128x16\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Conv2D(n, (1,1), kernel_initializer='he_normal')(x)\n",
    "for i in range(len(blocks)):\n",
    "    #Transition\n",
    "    if i != 0:\n",
    "        x, n = transition_layer(x, n)\n",
    "    #DenseBlock\n",
    "    x, n = dense_block(x, n, k, blocks[i])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "if load_weight:\n",
    "    model.load_weights(direc1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc1 = 'models/09_l_checkpoint.h5'\n",
    "direc2 = 'models/09_l_eval.txt'\n",
    "direc_out = 'submissions/submission09_l.csv'\n",
    "\n",
    "load_weight = True\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='auto', period=1)\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "#network\n",
    "xin = Input(shape=(512,512,4,))\n",
    "\n",
    "x = Conv2D(32, (5,5), padding='same')(xin)\n",
    "#x = Conv2D(32, (3,3), strides=(2,2), padding='same')(xin)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = Conv2D(64, (3,3), strides=(2,2), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = darknet_block(x, 64, LReLu_rate=0.1)\n",
    "x = darknet_block(x, 64, LReLu_rate=0.1)\n",
    "x = Conv2D(128, (3,3), strides=(2,2), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = darknet_block(x, 128, LReLu_rate=0.1)\n",
    "x = darknet_block(x, 128, LReLu_rate=0.1)\n",
    "x = Conv2D(256, (3,3), strides=(2,2), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = darknet_block(x, 256, LReLu_rate=0.1)\n",
    "x = darknet_block(x, 256, LReLu_rate=0.1)\n",
    "x = Conv2D(512, (3,3), strides=(2,2), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = darknet_block(x, 512, LReLu_rate=0.1)\n",
    "x = darknet_block(x, 512, LReLu_rate=0.1)\n",
    "x = Conv2D(1024, (3,3), strides=(2,2), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.1)(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model = Model(inputs=xin, outputs=y)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "if load_weight:\n",
    "    model.load_weights(direc1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc1 = 'models/10_l_checkpoint.h5'\n",
    "direc1_cross_entropy = 'models/10_cs_checkpoint.h5'\n",
    "direc2 = 'models/10_l_eval.txt'\n",
    "direc_out = 'submissions/submission10_l.csv'\n",
    "\n",
    "x_train = data['Id']\n",
    "y_train = data['Target']\n",
    "(x_train_mini, x_test_mini, y_train_mini, y_test_mini) = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "activation = 'relu'\n",
    "cb_es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n",
    "check_point = ModelCheckpoint(direc1, monitor='val_loss', \n",
    "                              verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='auto', period=1)\n",
    "ch = 16\n",
    "\n",
    "def base_network(xin):\n",
    "    x = Conv2D(ch, (5,5), strides=(2,2), padding='same')(xin)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    x = darknet_block(x, ch, LReLu_rate=0.1)\n",
    "    x = darknet_block(x, ch, LReLu_rate=0.1)\n",
    "    x = Conv2D(ch*2, (3,3), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    x = darknet_block(x, ch*2, LReLu_rate=0.1)\n",
    "    x = darknet_block(x, ch*2, LReLu_rate=0.1)\n",
    "    x = Conv2D(ch*4, (3,3), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    x = darknet_block(x, ch*4, LReLu_rate=0.1)\n",
    "    x = darknet_block(x, ch*4, LReLu_rate=0.1)\n",
    "    x = Conv2D(ch*8, (3,3), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return x\n",
    "\n",
    "#network1\n",
    "xing = Input(shape=(512,512,1,))\n",
    "xg = base_network(xing)\n",
    "\n",
    "#network2\n",
    "xinr = Input(shape=(512,512,1,))\n",
    "xr = base_network(xinr)\n",
    "\n",
    "#network3\n",
    "xinb = Input(shape=(512,512,1,))\n",
    "xb = base_network(xinb)\n",
    "\n",
    "#network4\n",
    "xiny = Input(shape=(512,512,1,))\n",
    "xy = base_network(xiny)\n",
    "\n",
    "x = Concatenate()([xg,xr,xb,xy])\n",
    "\n",
    "y = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0005)\n",
    "model = Model(inputs=[xing, xinr, xinb, xiny], outputs=y)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1])\n",
    "#model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "model.load_weights(direc1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
